# -*- coding: utf-8 -*-
"""Cuaderno: Sistemas de Recomendación por Paul Santos Ramos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nqVofTQwukbRn_LzAtn91ToTul8rkzSb

# Proyecto Final – Sistemas de Recomendación por Paul Santos Ramos

Este cuaderno de Google Colab recoge el desarrollo del proyecto final para la asignatura **Sistemas de Recomendación**, del **Máster Universitario en Ciberseguridad e Inteligencia de Datos** (Universidad de La Laguna, curso 2024/2025).

El objetivo principal es **evaluar y comparar distintos enfoques de recomendación de películas**, utilizando tanto métodos clásicos como algoritmos más avanzados:

- **Filtrado colaborativo basado en vecinos** (`KNN`), tanto por **usuarios** como por **ítems**, aplicando distintas métricas de similitud (*MSD*, *Cosine*, *Pearson*, *Pearson Baseline*).
- **Modelos basados en contenido**, que generan recomendaciones a partir de las características textuales de las películas (géneros y títulos), utilizando técnicas como `TF-IDF` y similitudes (*cosine*, *linear*, *euclidean*, *pearson*).
- **Factorización matricial con `SVD`**, que aprende factores latentes que representan usuarios e ítems a partir de sus valoraciones.
- **Redes neuronales profundas personalizadas**, construidas con `Keras`, que utilizan capas de `embedding` para representar usuarios, ítems y atributos adicionales (edad, ocupación, género, etc.), y redes densas para predecir valoraciones.

A lo largo del cuaderno se detalla la **preparación de los datos**, la **implementación de los modelos**, la **validación cruzada**, y un análisis de sensibilidad tipo *ceteris paribus* para evaluar el impacto individual de los hiperparámetros.

El conjunto de datos utilizado incluye **100.000 valoraciones de usuarios sobre películas**, junto con información adicional como edad, ocupación, género y géneros cinematográficos. Esta riqueza de datos permite construir modelos más complejos y evaluar su comportamiento en escenarios realistas.

##  Instalación de dependencias necesarias

Antes de ejecutar el código del proyecto, se lleva a cabo una preparación del entorno instalando manualmente versiones específicas de algunas librerías. Esto se hace por dos razones principales: evitar conflictos entre versiones y asegurar la compatibilidad con el resto del código.

###  Paso 1: Instalación de versiones compatibles

Se instalan versiones concretas de `pandas`, `scipy`, `scikit-learn` y `scikeras` para asegurar que todo el entorno esté alineado y funcione correctamente con los modelos implementados más adelante en el cuaderno.

###  Paso 2: Instalación de `scikit-surprise`

Se instala la biblioteca `scikit-surprise`, utilizada para trabajar con modelos clásicos de sistemas de recomendación (como SVD o KNN). La opción `--no-cache-dir` garantiza que se descargue una versión limpia y actualizada, evitando conflictos con posibles instalaciones anteriores en caché.
"""

# Paso 1: Instalar versiones compatibles de dependencias
# Removing the specific version constraint for scikit-learn
!pip install --no-cache-dir  pandas==1.5.3 scipy==1.10.1
!pip install --no-cache-dir scikit-learn==1.2.2 scikeras==0.11.0


# Paso 2: Instalar scikit-surprise sin caché para evitar conflictos binarios
!pip install --no-cache-dir scikit-surprise

"""## Montar Google Drive

Se monta Google Drive en el entorno de ejecución de Google Colab. Esto permite acceder a archivos almacenados en la cuenta personal del usuario como si fueran parte del sistema de archivos local del cuaderno. Es útil, por ejemplo, para cargar conjuntos de datos, guardar resultados o importar scripts directamente desde el Drive.

Al ejecutar este bloque, se solicitará al usuario que autorice el acceso a su cuenta de Google para vincular el Drive con el entorno de Colab.

"""

from google.colab import drive
drive.mount('/content/drive')

"""## Importación de librerías

En esta sección se importan las principales librerías necesarias para el desarrollo del proyecto:

- **Manipulación de datos**: Se utilizan `pandas` y `numpy` para cargar, explorar y transformar los datos. También se suprimen advertencias innecesarias para mantener limpio el entorno.
- **Visualización**: Se emplea `matplotlib` y `tabulate` para representar gráficamente los resultados y mostrar tablas con formato legible.
- **Surprise (Sistemas de recomendación clásicos)**: Se importa el paquete `surprise`, que permite trabajar con modelos de recomendación tradicionales como SVD, KNN básico y variantes basadas en baseline. También se incluyen funciones para validación cruzada y búsqueda de hiperparámetros.
- **Preprocesamiento y evaluación**: Se incluyen funciones de `scikit-learn` para dividir datos, codificar variables categóricas, y calcular métricas de evaluación como RMSE y MAE.
- **Deep Learning**: Se emplea `TensorFlow Keras` para definir y entrenar redes neuronales para sistemas de recomendación avanzados. Se integran también herramientas como `Scikeras` para facilitar su uso junto con `scikit-learn`.

"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Manipulación de datos y utilidades
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
import pandas as pd
import numpy as np
import datetime
import time
import warnings
from itertools import product
from tqdm import tqdm
warnings.filterwarnings("ignore", category=RuntimeWarning)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Visualización de datos
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
from tabulate import tabulate

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Preprocesamiento y evaluación
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Sistemas de recomendación clásicos (Surprise)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
from surprise import Dataset, Reader, SVD, KNNBasic, KNNBaseline
from surprise.model_selection import (
    cross_validate,
    GridSearchCV,
    train_test_split as surprise_train_test_split,
    KFold as SurpriseKFold
)
from surprise.trainset import Trainset

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Deep Learning (Keras y Scikeras)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from scikeras.wrappers import KerasRegressor

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Recomendación basada en contenido
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, euclidean_distances

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Guardado y carga de modelos
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
import joblib

"""## Carga y unión de datos

En esta sección se cargan los archivos del dataset **MovieLens 100K** desde el Google Drive. El dataset está dividido en tres archivos:

- `u.data`: contiene las valoraciones realizadas por los usuarios, incluyendo el `userId`, `movieId`, `rating` y `timestamp`.
- `u.item`: contiene información sobre las películas, como el `movieId`, título, fecha de estreno y una serie de columnas binarias indicando la pertenencia a diferentes géneros.
- `u.user`: contiene información sobre los usuarios, como edad, género, ocupación y código postal.

Los tres conjuntos se cargan como DataFrames de Pandas, y luego se unen en un único DataFrame (`df_datos`) usando la clave común `movieId` y `userId`. Finalmente, se muestra una vista previa de las primeras filas para verificar que la unión fue correcta.

"""

# Rutas locales para MovieLens 100K (ajusta si necesario)
ratings_path = '/content/drive/MyDrive/Movies/ml-100k/u.data'
movies_path = '/content/drive/MyDrive/Movies/ml-100k/u.item'
users_path = '/content/drive/MyDrive/Movies/ml-100k/u.user'

# Leer ratings: userId | movieId | rating | timestamp (tab-separated)
ratings = pd.read_csv(ratings_path, sep='\t', names=['userId', 'movieId', 'rating', 'timestamp'], encoding='latin1')

# Leer películas: movieId | title | release_date | video_release_date | imdb_url | + 19 géneros binarios
movies = pd.read_csv(movies_path, sep='|', header=None, encoding='latin1',
                     names=['movieId', 'title', 'release_date', 'video_release_date', 'imdb_url'] +
                           ['unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy', 'Crime',
                            'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
                            'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])

# Leer usuarios: userId | age | gender | occupation | zip
users = pd.read_csv(users_path, sep='|', names=['userId', 'age', 'gender', 'occupation', 'zip'], encoding='latin1')

# Unir todo en un solo DataFrame
df_datos = ratings.merge(movies, on='movieId').merge(users, on='userId')

# Mostrar las primeras filas
print(df_datos.head())

"""## Conversión de timestamps a fecha y hora legibles

En esta sección se transforma la columna `timestamp`, que representa el momento en que se realizó cada valoración (en segundos desde el 1 de enero de 1970), a un formato de fecha y hora legible utilizando `pd.to_datetime`.

Posteriormente, se extraen dos nuevas columnas:
- `solo_fecha`: contiene únicamente la fecha (año, mes y día) de la valoración.
- `solo_hora`: contiene únicamente la hora (hora, minuto y segundo) en la que se realizó la valoración.

Estas columnas permiten un análisis temporal más detallado y legible del comportamiento de los usuarios.

"""

df_datos['fecha'] = pd.to_datetime(df_datos['timestamp'], unit='s')
df_datos['solo_fecha'] = df_datos['fecha'].dt.date
df_datos['solo_hora'] = df_datos['fecha'].dt.time

print(f"A continuación se presenta la columna con solo la fecha de la valoración: {df_datos['solo_fecha'].head()}")
print(f"A continuación se presenta la columna con solo la hora de la valoración: {df_datos['solo_hora'].head()}")

"""### Preparación de los datos: limpieza y creación de columna de géneros

Primero, se eliminan del DataFrame original (`df_datos`) varias columnas que no son necesarias para el análisis, como las fechas de publicación y el identificador de tiempo (`timestamp`). El resultado se guarda en un nuevo DataFrame llamado `Datos`.

A continuación, se construye una nueva columna llamada `genres_text`. Esta columna contiene una representación en texto de los géneros asociados a cada película. Para ello, se recorre cada fila del DataFrame y se concatenan los nombres de los géneros que están activos (es decir, marcados con un 1). Esto facilita el análisis y la visualización posterior de los géneros de las películas en lugar de trabajar con múltiples columnas binarias.

"""

Datos = df_datos.drop(['timestamp', 'fecha', 'release_date', 'video_release_date', 'imdb_url'], axis=1).copy()


# Lista de columnas de géneros
genre_columns = ['unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy', 'Crime',
                 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
                 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']

# Crear columna 'genres_text' uniendo los géneros activados en cada fila
Datos['genres_text'] = Datos[genre_columns].apply(lambda row: ', '.join([genre for genre in genre_columns if row[genre] == 1]), axis=1)
Datos.head()

"""### Análisis de duplicados

En este bloque se lleva a cabo un análisis para detectar posibles valores duplicados en el conjunto de datos:

- Primero se comprueba si existen **filas completamente duplicadas** en el DataFrame (`Datos`). Es decir, filas donde todos los valores son idénticos.
- Luego se identifican duplicados basados únicamente en la combinación de `userId` y `movieId`, lo cual indica que un mismo usuario ha valorado más de una vez la misma película.
- Finalmente, se agrupan los datos por `userId` y `movieId` y se cuentan las repeticiones para mostrar explícitamente las combinaciones que aparecen más de una vez. Esto permite ver cuántas veces un usuario ha calificado una misma película, lo cual puede influir en la calidad del modelo si no se trata adecuadamente.

"""

duplicados_totales = Datos.duplicated()
print(f" Filas totalmente duplicadas: {duplicados_totales.sum()}")
duplicados_usuario_pelicula = Datos.duplicated(subset=['userId', 'movieId'])
print(f" Calificaciones duplicadas (mismo usuario y película): {duplicados_usuario_pelicula.sum()}")
conteo = Datos.groupby(['userId', 'movieId']).size().reset_index(name='repeticiones')
duplicados = conteo[conteo['repeticiones'] > 1]
print(f" Combinaciones userId-movieId repetidas:\n{duplicados}")

"""### Resultados del análisis de duplicados

Tras ejecutar el análisis de duplicados, se han obtenido los siguientes resultados:

- **Filas completamente duplicadas:** 0  
  No hay filas en el conjunto de datos que sean idénticas en todas sus columnas.

- **Calificaciones duplicadas (mismo usuario y película):** 0  
  Ningún usuario ha valorado la misma película más de una vez.

- **Combinaciones `userId`–`movieId` repetidas:** Ninguna  
  El DataFrame resultante está vacío, lo que confirma que cada combinación de usuario y película es única.

Este resultado asegura que no hay necesidad de realizar limpieza por duplicados antes de entrenar los modelos.

### Análisis general de las columnas del conjunto de datos

En esta celda se obtiene una visión general de cada columna del DataFrame `Datos`. Para ello, se recorre cada columna y se recogen los siguientes aspectos:

- **Nombre de la columna**
- **Tipo de dato** (por ejemplo, `int64`, `object`, etc.)
- **Número de valores no nulos**
- **Número de valores nulos**
- **Cantidad de valores únicos** (sin repetir)
- **Total de entradas en la columna**

Esta información es útil para:

- Detectar posibles columnas con valores nulos que requieran imputación.
- Comprobar la variedad de valores en variables categóricas o discretas.
- Verificar la consistencia y calidad del conjunto de datos antes de aplicar modelos de recomendación.
"""

data_info = []
for col in Datos.columns:
    data_info.append([
        col,
        str(Datos[col].dtype),
        Datos[col].notnull().sum(),
        Datos[col].isnull().sum(),
        Datos[col].nunique(),
        Datos[col].count()
    ])

# Crear DataFrame para la tabla de información
info_df = pd.DataFrame(data_info, columns=['Columna', 'Tipo de Dato', 'No Nulos', 'Nulos', 'Valores Únicos', 'Total'])

# Mostrar tabla formateada
print(tabulate(info_df, headers='keys', tablefmt='fancy_grid', numalign='right'))

"""### Descripción estructural del conjunto de datos

En la tabla anterior se presenta un resumen detallado de todas las columnas del DataFrame `Datos`, incluyendo variables numéricas, categóricas y de texto. Para cada columna se muestra:

- **Nombre de la columna**.
- **Tipo de dato** (`int64`, `object`, etc.).
- **Cantidad de valores no nulos**, confirmando que no existen datos faltantes en ninguna columna.
- **Cantidad de valores únicos**, que permite identificar la variedad de respuestas o categorías en cada campo.
- **Total de registros**, que en todos los casos es 100,000, correspondiente al número total de valoraciones en el dataset.

Esta inspección es clave para:

- Verificar la completitud y consistencia de los datos.
- Detectar posibles variables irrelevantes o de alta cardinalidad que podrían requerir transformación.
- Entender mejor la estructura general del dataset antes de aplicar técnicas de recomendación.

### Estadísticas y distribución de las valoraciones

En este bloque se presentan estadísticas descriptivas básicas sobre las valoraciones de películas realizadas por los usuarios. Se muestra información como la media, desviación estándar, valores mínimo y máximo, así como los percentiles.

Además, se visualiza la **distribución de frecuencias** de las valoraciones (de 1 a 5) mediante un gráfico de barras. Esta visualización permite identificar si existe sesgo hacia calificaciones más altas o más bajas, lo cual es útil para comprender el comportamiento general de los usuarios y ajustar los modelos de recomendación en consecuencia.
"""

# Estadísticas básicas
print(" Estadísticas de las valoraciones:")
print(Datos['rating'].describe())



plt.figure(figsize=(6,4))
sns.countplot(x='rating', data=Datos, palette='viridis')
plt.title('Distribución de valoraciones')
plt.xlabel('Rating')
plt.ylabel('Frecuencia')
plt.show()

"""### Distribución de Género y Ocupación

En este bloque de código se generan dos gráficos de barras que permiten visualizar la distribución de los usuarios según su **género** y su **ocupación** utilizando los datos reales del dataset:

- Se calcula la frecuencia de aparición de cada valor en las columnas `gender` y `occupation`.
- A continuación, se crea una figura con dos subgráficos:
  - El **primer gráfico** representa la cantidad de usuarios por género (por ejemplo, masculino y femenino).
  - El **segundo gráfico** representa la cantidad de usuarios por ocupación, ordenados de mayor a menor frecuencia.
- Estos gráficos ayudan a entender la composición demográfica de los usuarios del sistema de recomendación.

Ambas visualizaciones se generan con `seaborn` y `matplotlib`, facilitando así un análisis exploratorio inicial del dataset.

"""

# Calcular frecuencias desde el DataFrame original
genero = Datos['gender'].value_counts()
ocupacion = Datos['occupation'].value_counts()

# Crear figura con dos subgráficos
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de distribución por género
sns.barplot(x=genero.index, y=genero.values, ax=axes[0], palette="pastel")
axes[0].set_title('Distribución por género')
axes[0].set_ylabel('Número de usuarios')
axes[0].set_xlabel('Género')

# Gráfico de distribución por ocupación
ocupacion = ocupacion.sort_values(ascending=False)
sns.barplot(y=ocupacion.index, x=ocupacion.values, ax=axes[1], palette="muted")
axes[1].set_title('Distribución por ocupación')
axes[1].set_xlabel('Número de usuarios')
axes[1].set_ylabel('Ocupación')

plt.tight_layout()
plt.show()

"""### Distribución de Género y Ocupación de los Usuarios

Los gráficos presentan un análisis exploratorio de la población usuaria del sistema de recomendación:

- **Distribución por género:** Se observa un claro predominio de usuarios de género masculino, representando aproximadamente tres cuartas partes del total.
- **Distribución por ocupación:** La ocupación más frecuente entre los usuarios es "student", seguida de otras como "other", "educator", "engineer" y "programmer". Esto sugiere una base de usuarios mayoritariamente joven y técnica, lo cual puede influir en sus patrones de valoración de películas.

Este análisis proporciona contexto demográfico relevante que puede ayudar a interpretar los resultados de los modelos de recomendación.

## Sistema de Recomendación Colaborativo Basado en Usuarios (User-Based)

### Preparación del Dataset para los Modelos de Filtrado Colaborativo

En este bloque se prepara el dataset en el formato requerido por la librería `Surprise`, especializada en sistemas de recomendación:

- Se seleccionan únicamente las columnas necesarias: `userId`, `movieId` y `rating`, que representan las interacciones entre usuarios y películas.
- Se crea un objeto `Reader` para especificar el rango de puntuaciones posibles (en este caso, de 1 a 5).
- Finalmente, se utiliza `Dataset.load_from_df()` para transformar el DataFrame de pandas en un objeto `Dataset` que puede ser utilizado por los algoritmos de recomendación de `Surprise`, como SVD o KNN.

Este paso es fundamental para entrenar y evaluar modelos de filtrado colaborativo basados en las valoraciones de los usuarios.
"""

df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)

"""###  Evaluación de modelos colaborativos basados en usuarios

En este bloque de código se definen las **configuraciones óptimas de hiperparámetros** para el algoritmo `KNNBasic`, empleado en sistemas de recomendación colaborativos basados en usuarios. Se contemplan cuatro métricas de similitud distintas: `msd`, `cosine`, `pearson` y `pearson_baseline`.

Los parámetros seleccionados han sido obtenidos previamente tras un análisis sistemático mediante validación cruzada, evaluando diferentes combinaciones para minimizar el error de predicción (RMSE).

Una vez establecidas las configuraciones óptimas, se procede a evaluarlas utilizando validación cruzada con 3 particiones (`cv=3`). Durante este proceso se calculan las métricas de error **RMSE** y **MAE** para valorar el rendimiento de cada modelo.

Este procedimiento permite comparar de forma objetiva las distintas métricas de similitud y seleccionar el modelo más eficaz para el conjunto de datos MovieLens 100K.

"""

# Definir configuraciones óptimas para cada métrica de similitud
mejores_configuraciones = {
    'msd': {
        'k': 20,
        'min_k': 3,
        'sim_options': {
            'name': 'msd',
            'user_based': True,
            'shrinkage': 50,
            'min_support': 5
        }
    },
    'cosine': {
        'k': 50,
        'min_k': 3,
        'sim_options': {
            'name': 'cosine',
            'user_based': True,
            'shrinkage': 10,
            'min_support': 3
        }
    },
    'pearson': {
        'k': 60,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson',
            'user_based': True,
            'shrinkage': 10,
            'min_support': 3
        }
    },
    'pearson_baseline': {
        'k': 50,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson_baseline',
            'user_based': True,
            'shrinkage': 100,
            'min_support': 1
        }
    }
}

# Evaluar cada configuración usando validación cruzada
for nombre, config in mejores_configuraciones.items():
    print(f" Evaluando modelo KNNBasic con métrica: {nombre}")
    model = KNNBasic(k=config['k'], min_k=config['min_k'], sim_options=config['sim_options'])
    resultados = cross_validate(model, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)

"""###  Entrenamiento y guardado de modelos de recomendación (user-based)

Se entrenan y guardan cuatro modelos de filtrado colaborativo basado en usuarios (`KNNBasic`) con diferentes métricas de similitud:

- **Modelos**: `msd`, `cosine`, `pearson`, `pearson_baseline`
- **Todos** utilizan configuraciones óptimas predefinidas (k, min_k, shrinkage, etc.)
- Cada modelo se guarda como archivo `.pkl` con `joblib`




"""

# Crear trainset completo desde tus datos
df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)
trainset = data.build_full_trainset()

# Definir configuraciones óptimas (user-based)
mejores_configuraciones = {
    'msd': {
        'k': 20,
        'min_k': 3,
        'sim_options': {
            'name': 'msd',
            'user_based': True,
            'shrinkage': 50,
            'min_support': 5
        }
    },
    'cosine': {
        'k': 50,
        'min_k': 3,
        'sim_options': {
            'name': 'cosine',
            'user_based': True,
            'shrinkage': 10,
            'min_support': 3
        }
    },
    'pearson': {
        'k': 60,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson',
            'user_based': True,
            'shrinkage': 10,
            'min_support': 3
        }
    },
    'pearson_baseline': {
        'k': 50,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson_baseline',
            'user_based': True,
            'shrinkage': 100,
            'min_support': 1
        }
    }
}

# Entrenar y guardar
for nombre, config in mejores_configuraciones.items():
    print(f" Entrenando modelo '{nombre}'...")
    modelo = KNNBasic(k=config['k'], min_k=config['min_k'], sim_options=config['sim_options'])
    modelo.fit(trainset)
    joblib.dump(modelo, f"modelo_usuario_{nombre}.pkl")
    print(f" Modelo '{nombre}' guardado como 'modelo_usuario_{nombre}.pkl'")

"""###  Recomendaciones personalizadas con KNN (user-based)

Este código implementa un sistema de recomendación interactivo usando modelos `KNNBasic` de la librería `Surprise`, ya entrenados previamente. Su funcionamiento es el siguiente:

1. **Carga de datos**: Se utiliza un conjunto de datos de valoraciones de usuarios (`userId`, `movieId`, `rating`) para construir el conjunto de entrenamiento (`trainset`).

2. **Selección del modelo**: El usuario elige uno de los modelos previamente entrenados y guardados:  
   - `msd`  
   - `cosine`  
   - `pearson`  
   - `pearson_baseline`  
   Estos modelos se cargan desde archivos `.pkl`.

3. **Entrada de usuario**: Se solicita el `userId` y se valida que esté presente en los datos.

4. **Generación de recomendaciones**:
   - Se identifican las películas que el usuario no ha visto.
   - Se predicen las puntuaciones que el modelo estima que el usuario daría a cada una de esas películas.
   - Se ordenan y muestran las **10 películas más recomendadas**, junto con su puntuación predicha.

5. **Medición del rendimiento**:
   - Se calcula y muestra el **tiempo total** que tarda en hacer todas las predicciones.
   - También se informa del **tiempo medio por inferencia** (por película).

Este código permite probar fácilmente distintos algoritmos de recomendación basados en ítems y evaluar su eficiencia y calidad de recomendaciones de forma práctica e interactiva.

"""

# Crear trainset desde Datos
df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)
trainset = data.build_full_trainset()

# Modelos disponibles
modelos_disponibles = ['msd', 'cosine', 'pearson', 'pearson_baseline']

# Elegir modelo
while True:
    modelo_elegido = input(f"Elige un modelo entre {modelos_disponibles}: ").strip()
    if modelo_elegido in modelos_disponibles:
        break
    print(" Opción no válida.")

# Cargar modelo entrenado
modelo = joblib.load(f"modelo_usuario_{modelo_elegido}.pkl")
print(f" Modelo '{modelo_elegido}' cargado correctamente.")

# Predicciones interactivas
while True:
    uid = input("Introduce el userId (o 'salir' para terminar): ").strip()
    if uid.lower() == 'salir':
        break

    # Validar que uid sea un número entero y exista en los datos
    try:
        uid_int = int(uid)
    except ValueError:
        print(" El userId debe ser un número entero.")
        continue

    if uid_int not in df['userId'].unique():
        print(" userId no válido.")
        continue

    # Obtener películas vistas por el usuario
    peliculas_vistas = set(df[df['userId'] == uid_int]['movieId'])

    # Obtener todas las películas disponibles
    todas_peliculas = set(df['movieId'].unique())

    # Películas no vistas
    peliculas_no_vistas = list(todas_peliculas - peliculas_vistas)

    # Calcular tiempo de predicción
    inicio = time.time()
    predicciones = [
        (iid, modelo.predict(uid=str(uid_int), iid=str(iid)).est)
        for iid in peliculas_no_vistas
    ]
    fin = time.time()

    duracion_total = fin - inicio
    tiempo_medio = duracion_total / len(peliculas_no_vistas)

    # Ordenar por mayor predicción
    top_predicciones = sorted(predicciones, key=lambda x: x[1], reverse=True)[:10]

    # Mostrar títulos recomendados
    print(" Top 10 películas recomendadas:")
    for movie_id, score in top_predicciones:
        titulo = Datos[Datos["movieId"] == movie_id]["title"].values[0]
        print(f" {titulo} (predicción: {score:.2f})")

    print(f" Tiempo total de inferencia: {duracion_total:.4f} segundos")
    print(f" Tiempo medio por película: {tiempo_medio:.6f} segundos")

"""## Sistema de Recomendación Colaborativo Basado en Ítems (Item-Based)

### Evaluación de modelos colaborativos basados en ítems

Este bloque de código define y evalúa las mejores configuraciones para modelos de recomendación **basados en ítems** utilizando el algoritmo `KNNBasic` de la librería `Surprise`. Las configuraciones óptimas para cada métrica de similitud (MSD, Cosine, Pearson y Pearson Baseline) fueron seleccionadas previamente mediante análisis exhaustivos y validación cruzada.

Cada configuración especifica valores óptimos para los hiperparámetros `k`, `min_k`, `shrinkage` y `min_support`, así como el tipo de métrica de similitud utilizada. Además, se establece `user_based = False` para indicar que se trata de un enfoque **item-based**.

El código recorre todas las configuraciones almacenadas en el diccionario `mejores_configuraciones_items`, y para cada una:
- Construye un modelo con `KNNBasic`.
- Evalúa su rendimiento mediante **validación cruzada** (`cross_validate`) utilizando 3 particiones (`cv=3`).
- Informa del error medio (RMSE y MAE) de cada modelo.

Este procedimiento es muy similar al aplicado en el caso de los modelos **basados en usuarios**, con la diferencia de que aquí se emplea la información de los ítems (películas) en lugar de los usuarios como referencia principal para encontrar similitudes y realizar recomendaciones.
"""

# Definir configuraciones óptimas para cada métrica de similitud (basado en ítems)
mejores_configuraciones_items = {
    'msd': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'msd',
            'user_based': False,
            'shrinkage': 10,
            'min_support': 1
        }
    },
    'cosine': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'cosine',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 2
        }
    },
    'pearson': {
        'k': 80,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 1
        }
    },
    'pearson_baseline': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson_baseline',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 1
        }
    }
}

# Evaluar cada configuración usando validación cruzada
for nombre, config in mejores_configuraciones_items.items():
    print(f" Evaluando modelo KNNBasic (item-based) con métrica: {nombre}")
    modelo = KNNBasic(k=config['k'], min_k=config['min_k'], sim_options=config['sim_options'])
    resultados = cross_validate(modelo, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)

"""###  Entrenamiento y guardado de modelos de recomendación (item-based)

Este código entrena y guarda varios modelos de recomendación basados en ítems (`item-based`) usando `KNNBasic` de la librería `Surprise`.

- Se parte de un dataset de valoraciones de usuarios (`userId`, `movieId`, `rating`).
- Se definen configuraciones óptimas para cuatro métricas de similitud:
  - `msd`
  - `cosine`
  - `pearson`
  - `pearson_baseline`
- Para cada configuración:
  - Se entrena un modelo con `KNNBasic`.
  - Se guarda el modelo entrenado en un archivo `.pkl` con el nombre correspondiente (`modelo_item_*.pkl`).

Este proceso permite tener modelos listos para hacer recomendaciones personalizadas.


"""

# Crear trainset desde Datos
df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)
trainset = data.build_full_trainset()

# Definir las configuraciones óptimas
mejores_configuraciones_items = {
    'msd': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'msd',
            'user_based': False,
            'shrinkage': 10,
            'min_support': 1
        }
    },
    'cosine': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'cosine',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 2
        }
    },
    'pearson': {
        'k': 80,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 1
        }
    },
    'pearson_baseline': {
        'k': 40,
        'min_k': 3,
        'sim_options': {
            'name': 'pearson_baseline',
            'user_based': False,
            'shrinkage': 100,
            'min_support': 1
        }
    }
}

# Entrenar y guardar cada modelo
for nombre_modelo, config in mejores_configuraciones_items.items():
    print(f" Entrenando modelo '{nombre_modelo}'...")
    modelo = KNNBasic(k=config['k'], min_k=config['min_k'], sim_options=config['sim_options'])
    modelo.fit(trainset)
    ruta = f"modelo_item_{nombre_modelo}.pkl"
    joblib.dump(modelo, ruta)
    print(f" Modelo '{nombre_modelo}' guardado en '{ruta}'")

"""###  Recomendaciones personalizadas con KNN (item-based)

Este script permite al usuario generar recomendaciones personalizadas utilizando un modelo `KNNBasic` entrenado previamente con una de las siguientes métricas de similitud:

- `msd`
- `cosine`
- `pearson`
- `pearson_baseline`

#### Funcionalidad:
1.  Se cargan los datos de usuario y películas.
2.  Se carga el modelo elegido por el usuario.
3.  Se solicita un `userId` y se validan las entradas.
4.  Se calculan predicciones para las películas no vistas por el usuario.
5.  Se muestran las 10 películas con mejor puntuación estimada.
6.  Se informa del tiempo total y medio por inferencia.

Esto permite probar diferentes configuraciones y observar el rendimiento real en términos de recomendación y eficiencia.

"""

# Cargar datos y construir trainset
df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)
trainset = data.build_full_trainset()

# Modelos disponibles
modelos_disponibles = ['msd', 'cosine', 'pearson', 'pearson_baseline']

# Solicitar modelo al usuario
while True:
    modelo_item_elegido = input(f"Elige un modelo basado en ítems {modelos_disponibles}: ").strip()
    if modelo_item_elegido in modelos_disponibles:
        break
    else:
        print(" Opción no válida. Elige una opción correcta.")

# Cargar modelo correspondiente
modelo_item = joblib.load(f"modelo_item_{modelo_item_elegido}.pkl")
print(f" Modelo '{modelo_item_elegido}' cargado correctamente.")

# Predicciones automáticas para items no vistos
while True:
    uid = input("Introduce el userId (o 'salir' para terminar): ").strip()
    if uid.lower() == 'salir':
        break

    # Validar userId
    if not uid.isdigit() or int(uid) not in df['userId'].unique():
        print(" userId no válido.")
        continue

    uid = int(uid)

    # Obtener películas vistas por el usuario
    peliculas_vistas = set(df[df['userId'] == uid]['movieId'])

    # Películas no vistas
    peliculas_no_vistas = list(set(df['movieId'].unique()) - peliculas_vistas)

    # Tiempo de inferencia
    t_inicio = time.time()
    predicciones = [
        (iid, modelo_item.predict(uid=uid, iid=iid).est)
        for iid in peliculas_no_vistas
    ]
    t_fin = time.time()

    duracion_total = t_fin - t_inicio
    tiempo_medio = duracion_total / len(peliculas_no_vistas)

    # Ordenar por mayor predicción
    top_predicciones = sorted(predicciones, key=lambda x: x[1], reverse=True)[:10]

    # Mostrar recomendaciones
    print(" Top 10 películas recomendadas:")
    for movie_id, score in top_predicciones:
        titulo = Datos[Datos["movieId"] == movie_id]["title"].values[0]
        print(f" {titulo} (predicción: {score:.2f})")

    print(f" Tiempo total de inferencia: {duracion_total:.4f} segundos")
    print(f" Tiempo medio por película: {tiempo_medio:.6f} segundos")

"""##Evaluación del modelo SVD como sistema recomendador basado en descomposición

###  Evaluación de SVD

Este experimento aplica el mismo enfoque metodológico utilizado previamente en otros modelos (colaborativos y de deep learning), basado en un análisis **ceteris paribus**.

- Se parte de **4 configuraciones base** para el modelo SVD.
- Se evalúa el efecto individual de cada hiperparámetro:  
  - `n_factors` (factores latentes)  
  - `n_epochs` (épocas de entrenamiento)  
  - `lr_all` (tasa de aprendizaje)  
  - `reg_all` (coeficiente de regularización)  
- Por cada combinación se calcula el **RMSE** y el **tiempo de ejecución** usando validación cruzada (CV=3).
- Los resultados se almacenan y se muestran como un `DataFrame`.

Este proceso permite analizar de forma sistemática el rendimiento del modelo y su coste computacional, identificando los valores más efectivos para cada parámetro.
"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Preparación de los datos
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df = Datos[['userId', 'movieId', 'rating']].copy()
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Función para evaluar modelo SVD
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def evaluar_svd(n_factors, n_epochs, lr_all, reg_all):
    modelo = SVD(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all)
    inicio = time.time()
    resultados = cross_validate(modelo, data, measures=['RMSE'], cv=3, verbose=False)
    fin = time.time()
    return np.mean(resultados['test_rmse']), fin - inicio

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Definición de los hiperparámetros
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
rangos = {
    'n_factors': [100, 120, 150],
    'n_epochs': [20, 30, 40],
    'lr_all': [0.001, 0.005, 0.01],
    'reg_all': [0.05, 0.075, 0.1]
}

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Grid Search: probar todas las combinaciones posibles
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
resultados = []

for n_factors, n_epochs, lr_all, reg_all in product(
    rangos['n_factors'], rangos['n_epochs'], rangos['lr_all'], rangos['reg_all']
):
    rmse, tiempo = evaluar_svd(n_factors, n_epochs, lr_all, reg_all)
    resultados.append({
        'n_factors': n_factors,
        'n_epochs': n_epochs,
        'lr_all': lr_all,
        'reg_all': reg_all,
        'RMSE': rmse,
        'tiempo (s)': tiempo
    })

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Mostrar mejores combinaciones
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_resultados = pd.DataFrame(resultados)
mejor = df_resultados.sort_values('RMSE').head(1)

print(" Top 10 combinaciones ordenadas por RMSE:")
print(df_resultados.sort_values('RMSE').head(10).to_string(index=False))

print(" Mejor combinación encontrada:")
print(mejor.to_string(index=False))

"""### Recomendaciones con SVD usando parámetros óptimos

Este bloque de código entrena un modelo de recomendación basado en **SVD (Singular Value Decomposition)** utilizando la mejor combinación de hiperparámetros encontrada previamente (`n_factors=150`, `n_epochs=40`, `lr_all=0.01`, `reg_all=0.1`). El modelo se entrena con todos los datos disponibles.



"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Preparación de los datos
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Use the original 'Datos' DataFrame which contains the 'title' column
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(Datos[['userId', 'movieId', 'rating']], reader) # Still use subset for Surprise
trainset: Trainset = data.build_full_trainset()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Entrenamiento del modelo con parámetros óptimos
# (modificar más adelante según resultados de tuning)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
mejor_modelo = SVD(
    n_factors=150,
    n_epochs=40,
    lr_all=0.01,
    reg_all=0.1
)
mejor_modelo.fit(trainset)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Obtener películas no vistas por el usuario y predecir
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def recomendar_svd(user_id, modelo, trainset, df_with_title, n=5): # Changed df to df_with_title for clarity
    # Películas vistas por el usuario
    items_vistos = df_with_title[df_with_title['userId'] == user_id]['movieId'].unique()

    # Películas no vistas
    items_todos = df_with_title['movieId'].unique()
    items_no_vistos = [iid for iid in items_todos if iid not in items_vistos]

    # Predecir rating para cada ítem no visto
    predicciones = [
        (iid, modelo.predict(user_id, iid).est)
        for iid in items_no_vistos
    ]

    # Ordenar por predicción descendente
    predicciones_ordenadas = sorted(predicciones, key=lambda x: x[1], reverse=True)[:n]

    # Mapear a títulos si están disponibles
    peliculas = df_with_title[['movieId', 'title']].drop_duplicates().set_index('movieId')
    recomendaciones = [(peliculas.loc[iid]['title'], score) for iid, score in predicciones_ordenadas if iid in peliculas.index]

    return recomendaciones

"""Una vez entrenado, se generan recomendaciones personalizadas para un usuario específico. Para ello, se predice la puntuación esperada que el usuario daría a las películas que aún no ha visto, y se devuelven las 5 con mayor puntuación estimada.

El resultado final muestra las películas recomendadas y sus valores de predicción.
"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Recomendaciones para un usuario específico
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
usuario_objetivo = int(input(" Introduce el ID del usuario para recomendar películas: "))
top_recomendaciones = recomendar_svd(usuario_objetivo, mejor_modelo, trainset, Datos, n=5) # Passed the original Datos DataFrame

print(f" Recomendaciones para el usuario {usuario_objetivo}:")
for i, (titulo, pred) in enumerate(top_recomendaciones, 1):
    print(f"{i}. {titulo} — Predicción: {pred:.2f}")

"""## Sistema de Recomendación Basado en Contenido

###  Evaluación de sistemas basados en contenido con diferentes similitudes

Este bloque de código implementa y evalúa un sistema de recomendación **basado en contenido**, que utiliza la descripción textual de las películas (géneros y títulos) para calcular similitudes entre ítems y predecir valoraciones.

####  Funciones auxiliares

- `obtener_similitud(matriz, metodo)`: Calcula la matriz de similitud entre películas usando distintos métodos: `cosine`, `linear`, `euclidean` o `pearson`.
- `evaluar_similitud(...)`: Evalúa el rendimiento predictivo de cada método de similitud usando **validación cruzada** (KFold con 3 divisiones) y diferentes valores de vecinos más cercanos (`top_k`).

####  Preprocesamiento de datos

- Se combinan los géneros y títulos de cada película en un texto único.
- Se vectoriza este texto usando **TF-IDF**, lo que permite representar cada película como un vector numérico en un espacio de características.
- Se construye un mapeo entre `movieId` y su índice correspondiente en la matriz TF-IDF.

####  Evaluación de métodos de similitud

- Se prueban cuatro métodos de similitud:
  - `cosine`
  - `linear` (producto punto lineal)
  - `euclidean` (distancia euclídea transformada en similitud)
  - `pearson` (correlación de Pearson)
- Se prueban múltiples valores de `top_k` (número de vecinos): desde 5 hasta 50, además del caso especial `top_k=None` (que considera todos los vecinos disponibles).
- Para cada combinación se calculan métricas de error **RMSE** y **MAE**.

####  Resultados y visualización

- Los resultados se almacenan en un DataFrame y se muestran en formato de tabla para cada método.
- Finalmente, se genera un gráfico que compara el **RMSE en función de `top_k`** para cada método de similitud.

Este enfoque permite identificar qué tipo de similitud y cuántos vecinos producen las mejores recomendaciones en un sistema **basado en el contenido textual de las películas**.
"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# FUNCIONES AUXILIARES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

def obtener_similitud(matriz, metodo):
    if metodo == 'cosine':
        return cosine_similarity(matriz, dense_output=False)
    elif metodo == 'linear':
        return linear_kernel(matriz)
    elif metodo == 'euclidean':
        dists = euclidean_distances(matriz)
        return 1 / (1 + dists)
    elif metodo == 'pearson':
        densa = matriz.toarray()
        return np.corrcoef(densa)
    else:
        raise ValueError(f"Similitud desconocida: {metodo}")

def evaluar_similitud(datos, peliculas, similitud, movieId_to_index, top_ks, metodo_nombre):
    datos_filtrados = datos[datos['movieId'].isin(movieId_to_index)][['userId', 'movieId', 'rating']].copy()
    datos_filtrados['movie_idx'] = datos_filtrados['movieId'].map(movieId_to_index)

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    resultados = []

    for top_k in top_ks:
        rmse_scores, mae_scores = [], []

        for train_idx, test_idx in kf.split(datos_filtrados):
            train_df = datos_filtrados.iloc[train_idx]
            test_df = datos_filtrados.iloc[test_idx]

            predichos, reales = [], []
            grouped_train = train_df.groupby('userId')
            grouped_test = test_df.groupby('userId')

            for uid, test_user_ratings in grouped_test:
                if uid not in grouped_train.groups:
                    continue

                train_user_ratings = grouped_train.get_group(uid)
                movie_indices = train_user_ratings['movie_idx'].values
                user_ratings = train_user_ratings['rating'].values

                for row in test_user_ratings.itertuples(index=False):
                    if pd.isna(row.movie_idx):
                        continue

                    target_idx = row.movie_idx
                    if isinstance(similitud, np.ndarray):
                        sims = similitud[target_idx, movie_indices]
                    else:
                        sims = similitud[target_idx, movie_indices].toarray().flatten()

                    if top_k:
                        top_indices = np.argsort(sims)[-top_k:]
                        sims = sims[top_indices]
                        selected_ratings = user_ratings[top_indices]
                    else:
                        selected_ratings = user_ratings

                    if sims.sum() == 0 or len(selected_ratings) == 0:
                        pred = user_ratings.mean()
                    else:
                        pred = np.clip(np.dot(sims, selected_ratings) / sims.sum(), 1, 5)


                    predichos.append(pred)
                    reales.append(row.rating)

            if predichos:
                rmse_scores.append(np.sqrt(mean_squared_error(reales, predichos)))
                mae_scores.append(mean_absolute_error(reales, predichos))

        resultados.extend([{
            'metodo': metodo_nombre,
            'top_k': 'all' if top_k is None else top_k,
            'rmse': np.mean(rmse_scores),
            'mae': np.mean(mae_scores)
        }])

    return resultados

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DATOS Y VECTORIZACIÓN
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

datos = Datos.copy()
datos['texto_combinado'] = datos['genres_text'].fillna('') + ' ' + datos['title'].fillna('')
peliculas = datos[['movieId', 'title', 'texto_combinado']].drop_duplicates().reset_index(drop=True)

tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(peliculas['texto_combinado'])
movieId_to_index = dict(zip(peliculas['movieId'], peliculas.index))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUACIÓN DE DIFERENTES MÉTODOS DE SIMILITUD
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

metodos = ['cosine', 'linear', 'euclidean', 'pearson']
top_ks = [None] + list(range(5, 51))


resultados_totales = []
for metodo in metodos:
    print(f"\nEvaluando método: {metodo}")
    sim = obtener_similitud(tfidf_matrix, metodo)
    res = evaluar_similitud(datos, peliculas, sim, movieId_to_index, top_ks, metodo)
    resultados_totales.extend(res)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# MOSTRAR RESULTADOS Y GRAFICAR
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

df_resultados = pd.DataFrame(resultados_totales)

# Mostrar tabla por método
for metodo in df_resultados['metodo'].unique():
    tabla = df_resultados[df_resultados['metodo'] == metodo][['top_k', 'rmse', 'mae']]
    print(f" Resultados para el método '{metodo}':")
    print(tabulate(tabla, headers='keys', tablefmt='grid', showindex=False))

orden_k = [str(k) for k in range(5, 51)] + ['all']
df_resultados['top_k'] = df_resultados['top_k'].astype(str)
df_resultados['top_k'] = pd.Categorical(df_resultados['top_k'], categories=orden_k, ordered=True)



plt.figure(figsize=(10, 6))

# Graficar todos los puntos
for metodo in df_resultados['metodo'].unique():
    df_met = df_resultados[df_resultados['metodo'] == metodo].sort_values('top_k')
    plt.plot(df_met['top_k'], df_met['rmse'], marker='o', label=metodo)

# Etiquetas espaciadas en el eje X
top_k_values = sorted(df_resultados['top_k'].unique())
salto = max(1, len(top_k_values) // 6)  # Mostrar ~6 etiquetas en X
indices_a_mostrar = top_k_values[::salto]
plt.xticks(indices_a_mostrar)

plt.title("Comparación de RMSE por número de vecinos (top_k)")
plt.xlabel("Número de vecinos (top_k)")
plt.ylabel("RMSE")
plt.legend(title="Similitud")
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Búsqueda de hiperparámetro `top_k` para métodos de similitud

Este código realiza una búsqueda en cuadrícula (`Grid Search`) sobre un conjunto reducido de valores posibles para el hiperparámetro `top_k`, el cual representa el número de vecinos considerados al hacer recomendaciones basadas en contenido.

Para cada una de las métricas de similitud (`cosine`, `linear`, `euclidean` y `pearson`), se calcula el error (RMSE y MAE) utilizando validación cruzada, y se selecciona la mejor configuración (es decir, el valor de `top_k` que minimiza el RMSE).

Finalmente, se imprime una tabla con la mejor configuración encontrada para cada método de similitud.

"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# RANGOS SIMPLIFICADOS DE top_k POR MÉTRICA
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
rango_top_k_por_metodo = {
    'cosine': [30, 35, 40, 45, 50],
    'linear': [30, 35, 40, 45, 50],
    'euclidean': [35, 40, 45, 50],
    'pearson': [40, 45, 50]
}

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# BÚSQUEDA EN CUADRÍCULA Y MEJOR MODELO POR MÉTODO
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
mejores_modelos = []

for metodo in rango_top_k_por_metodo:
    print(f"\nEvaluando método: {metodo}")

    sim = obtener_similitud(tfidf_matrix, metodo)
    top_ks = rango_top_k_por_metodo[metodo]

    res = evaluar_similitud(datos, peliculas, sim, movieId_to_index, top_ks, metodo)
    df_metodo = pd.DataFrame(res)

    mejor_fila = df_metodo.loc[df_metodo['rmse'].idxmin()]
    mejores_modelos.append(mejor_fila)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# MOSTRAR LOS MEJORES RESULTADOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_mejores = pd.DataFrame(mejores_modelos)
df_mejores = df_mejores[['metodo', 'top_k', 'rmse', 'mae']].sort_values('rmse')

print(" Mejor configuración por método de similitud:")
print(tabulate(df_mejores, headers='keys', tablefmt='grid', showindex=False))

"""###  Función de recomendación basada en contenido

La función `recomendar_peliculas` genera recomendaciones personalizadas para un usuario concreto utilizando un enfoque **basado en contenido** y **similitud entre películas**.

####  Lógica del algoritmo

1. **Películas vistas**:
   - Se identifican las películas que el usuario ya ha valorado (`peliculas_vistas`) y se excluyen del conjunto de posibles recomendaciones.

2. **Similitud con películas no vistas**:
   - Para cada película no vista por el usuario:
     - Se calcula la similitud con las películas que ya ha visto, usando la matriz de similitud (`similitud`), previamente calculada con métodos como `cosine`, `linear`, `euclidean` o `pearson`.
     - Se seleccionan los `top_k` vecinos más similares y se calcula la predicción de puntuación como una **media ponderada** de las valoraciones que el usuario ha hecho sobre esos ítems similares.

3. **Generación de recomendaciones**:
   - Se almacenan las predicciones para cada película no vista y se ordenan de mayor a menor.
   - Finalmente, se muestran las **top `n_recomendaciones`** con mayor puntuación estimada.

####  Parámetros

- `user_id`: ID del usuario para el que se desea hacer recomendaciones.
- `datos`: DataFrame con las valoraciones del usuario (`userId`, `movieId`, `rating`).
- `peliculas`: DataFrame con información de todas las películas.
- `similitud`: Matriz de similitud entre películas (puede ser densa o dispersa).
- `movieId_to_index`: Diccionario que mapea `movieId` a índices de la matriz de similitud.
- `top_k`: Número de vecinos más similares que se usan en el cálculo de predicción (por defecto, 20).
- `n_recomendaciones`: Número de películas a recomendar (por defecto, 5).

####  Resultado

Devuelve una lista con las películas recomendadas y su puntuación estimada.

Este enfoque es útil cuando se dispone de **información descriptiva de las películas** (como título y géneros), y se quiere ofrecer recomendaciones incluso en ausencia de valoraciones explícitas de otros usuarios.

"""

def recomendar_peliculas(user_id, datos, peliculas, similitud, movieId_to_index, top_k=20, n_recomendaciones=5):
    # Obtener películas ya vistas por el usuario
    peliculas_vistas = datos[datos['userId'] == user_id]['movieId'].unique()
    peliculas_no_vistas = peliculas[~peliculas['movieId'].isin(peliculas_vistas)].copy()
    datos_usuario = datos[datos['userId'] == user_id]

    if datos_usuario.empty:
        print(f" El usuario {user_id} no tiene valoraciones registradas.")
        return []

    # Mapear índices de películas vistas por el usuario
    movie_indices = datos_usuario['movieId'].map(movieId_to_index).dropna().astype(int).values
    user_ratings = datos_usuario['rating'].values

    recomendaciones = []

    for _, row in peliculas_no_vistas.iterrows():
        target_id = row['movieId']
        if target_id not in movieId_to_index:
            continue

        target_idx = movieId_to_index[target_id]

        # Obtener similitudes con películas vistas
        if isinstance(similitud, np.ndarray):
            sims = similitud[target_idx, movie_indices]
        else:
            sims = similitud[target_idx, movie_indices].toarray().flatten()

        if top_k:
            top_indices = np.argsort(sims)[-top_k:]
            sims = sims[top_indices]
            selected_ratings = user_ratings[top_indices]
        else:
            selected_ratings = user_ratings

        if sims.sum() == 0 or len(selected_ratings) == 0:
            pred = user_ratings.mean()  # Fallback
        else:
            pred = np.dot(sims, selected_ratings) / sims.sum()

        pred = np.clip(pred, 1, 5)  # ← Límite entre 1 y 5

        recomendaciones.append((row['title'], pred))

    # Ordenar y mostrar top-N
    recomendaciones = sorted(recomendaciones, key=lambda x: x[1], reverse=True)[:n_recomendaciones]

    return recomendaciones

"""### Generación de recomendaciones personalizadas (por similitud elegida)

Este bloque de código genera recomendaciones personalizadas para un usuario específico mediante un sistema de recomendación basado en contenido. A diferencia de versiones anteriores, el usuario puede seleccionar una única métrica de similitud para realizar la recomendación. El proceso sigue los siguientes pasos:

---

#### 1. Selección del usuario y métrica de similitud
Se solicita al usuario que introduzca su `userId`, así como la métrica de similitud a emplear entre las disponibles: `cosine`, `linear`, `euclidean` o `pearson`.

---

#### 2. Identificación de películas favoritas
Se extraen las 5 películas mejor valoradas por ese usuario, ordenadas de forma descendente según su puntuación. Estas sirven de referencia para contextualizar la recomendación.

---

#### 3. Cálculo de recomendaciones
Utilizando la métrica de similitud seleccionada, se construye una matriz de similitud basada en TF-IDF. Con dicha matriz, se ejecuta la función `recomendar_peliculas`, que predice las puntuaciones esperadas de las películas aún no vistas por el usuario, y selecciona las 5 con mayor predicción.

---

#### 4. Construcción de tabla resumen
Se genera una tabla que muestra:
- Las 5 recomendaciones obtenidas mediante el método de similitud elegido.

---

#### 5. Visualización
La tabla final se imprime con `tabulate`, facilitando la interpretación de las recomendaciones generadas para el usuario y el método elegido.

"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Solicitar al usuario el ID y la similitud
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
user_id_objetivo = int(input("Introduce el ID del usuario para generar recomendaciones: "))

similitudes_disponibles = ['cosine', 'linear', 'euclidean', 'pearson']
print(f"Similitudes disponibles: {', '.join(similitudes_disponibles)}")
metodo_seleccionado = input("Introduce la métrica de similitud a usar: ").strip().lower()

if metodo_seleccionado not in similitudes_disponibles:
    raise ValueError(f"Similitud no válida. Elige entre: {', '.join(similitudes_disponibles)}")

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# top_k específico por método
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
top_k_por_metodo = {
    'cosine': 30,
    'linear': 40,
    'euclidean': 35,
    'pearson': 20
}

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Obtener matriz de similitud seleccionada
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
sim = obtener_similitud(tfidf_matrix, metodo=metodo_seleccionado)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Generar recomendaciones
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
recs_titles = recomendar_peliculas(
    user_id=user_id_objetivo,
    datos=datos,
    peliculas=peliculas,
    similitud=sim,
    movieId_to_index=movieId_to_index,
    top_k=top_k_por_metodo[metodo_seleccionado],
    n_recomendaciones=5
)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Mostrar solo recomendaciones
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
print(f"\nRecomendaciones para el usuario {user_id_objetivo} usando similitud {metodo_seleccionado.capitalize()}:\n")
for i, (titulo, prediccion) in enumerate(recs_titles, start=1):
    print(f"{i}. {titulo} — Predicción: {prediccion:.2f}")

"""## Sistema de Recomendación Basado en Embeddings con Deep Learning

Se construyó un modelo de red neuronal que emplea representaciones embebidas (embeddings) para codificar información de usuarios, ítems, géneros, títulos, ocupación y edad. Estos vectores se combinan y se procesan a través de capas densas con funciones de activación (`relu`, `tanh`, `sigmoid`) y técnicas de regularización como `Dropout` y L2.

Los datos fueron preparados aplicando codificadores (`LabelEncoder`) y normalizando la edad. El modelo fue evaluado mediante validación cruzada (`KFold`) con tres particiones, calculando métricas de rendimiento como el RMSE y el MAE, así como el tiempo medio de inferencia por muestra.

Se analizaron distintas configuraciones óptimas según la función de activación utilizada, lo que permitió identificar combinaciones de hiperparámetros eficaces y eficientes en términos de precisión y coste computacional.

"""

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Preparar datos
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
datos = Datos[['userId', 'movieId', 'rating', 'genres_text', 'title', 'age', 'occupation']].copy()

user_enc = LabelEncoder()
item_enc = LabelEncoder()
genre_enc = LabelEncoder()
title_enc = LabelEncoder()
occ_enc = LabelEncoder()

datos["user"] = user_enc.fit_transform(datos["userId"])
datos["item"] = item_enc.fit_transform(datos["movieId"])
datos["genre"] = genre_enc.fit_transform(datos["genres_text"].fillna("Desconocido"))
datos["title_id"] = title_enc.fit_transform(datos["title"])
datos["occupation_id"] = occ_enc.fit_transform(datos["occupation"])
datos["age_norm"] = (datos["age"] - datos["age"].mean()) / datos["age"].std()

n_user_enc = datos["user"].nunique()
n_item_enc = datos["item"].nunique()
n_genres = datos["genre"].nunique()
n_titles = datos["title_id"].nunique()
n_occs = datos["occupation_id"].nunique()

X_user = datos["user"].values
X_item = datos["item"].values
X_genre = datos["genre"].values
X_title = datos["title_id"].values
X_occ = datos["occupation_id"].values
X_age = datos["age_norm"].values.reshape(-1, 1)  # si se usa

y = datos["rating"].values

# Configuraciones base óptimas según la activación
bases_por_activacion = {
    "relu": [
        {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.001, "dropout": 0.2},
        {"embedding_dim": 8, "hidden_units": 128, "learning_rate": 0.0005, "dropout": 0.3}
    ],
    "tanh": [
        {"embedding_dim": 32, "hidden_units": 256, "learning_rate": 0.001, "dropout": 0.3},
        {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.0005, "dropout": 0.4}
    ],
    "sigmoid": [
        {"embedding_dim": 32, "hidden_units": 256, "learning_rate": 0.0005, "dropout": 0.3},
        {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.001, "dropout": 0.2}
    ]
}


# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Modelo actualizado
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def build_model(n_users, n_items, n_genres, n_titles, n_occs,
                embedding_dim, hidden_units, lr, act, dropout_rate):

    user_input = Input(shape=(1,))
    item_input = Input(shape=(1,))
    genre_input = Input(shape=(1,))
    title_input = Input(shape=(1,))
    occ_input = Input(shape=(1,))
    age_input = Input(shape=(1,))  # opcional

    user_emb = Embedding(n_users, embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)
    item_emb = Embedding(n_items, embedding_dim, embeddings_regularizer=l2(1e-6))(item_input)
    genre_emb = Embedding(n_genres, embedding_dim, embeddings_regularizer=l2(1e-6))(genre_input)
    title_emb = Embedding(n_titles, embedding_dim, embeddings_regularizer=l2(1e-6))(title_input)
    occ_emb = Embedding(n_occs, embedding_dim, embeddings_regularizer=l2(1e-6))(occ_input)

    user_vec = Flatten()(user_emb)
    item_vec = Flatten()(item_emb)
    genre_vec = Flatten()(genre_emb)
    title_vec = Flatten()(title_emb)
    occ_vec = Flatten()(occ_emb)
    age_vec = age_input  # opcional (no embedding)

    x = Concatenate()([user_vec, item_vec, genre_vec, title_vec, occ_vec, age_vec])
    x = Dense(hidden_units, activation=act)(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(hidden_units // 2, activation=act)(x)
    x = Dropout(dropout_rate)(x)
    output = Dense(1)(x)

    model = Model(inputs=[user_input, item_input, genre_input, title_input, occ_input,  age_input],
                  outputs=output)
    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')
    return model

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Entrenamiento con validación cruzada
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
kf = KFold(n_splits=3, shuffle=True, random_state=42)

activ = ['relu', 'tanh', 'sigmoid']
resultados_cp = []

from tabulate import tabulate
for act in activ:
  for idx, base in enumerate(bases_por_activacion[act]):
      print(f" Evaluando configuración base C{idx+1} (activación: {act})")

      params = base.copy()
      params["activation"] = act

      rmse_folds = []
      mae_folds = []

      for train_idx, val_idx in kf.split(X_user):
          model = build_model(
              n_user_enc, n_item_enc, n_genres, n_titles, n_occs,
              embedding_dim=params["embedding_dim"],
              hidden_units=params["hidden_units"],
              lr=params["learning_rate"],
              act=params["activation"],
              dropout_rate=params["dropout"]
          )

          model.fit(
              [X_user[train_idx], X_item[train_idx], X_genre[train_idx],
              X_title[train_idx], X_occ[train_idx], X_age[train_idx]],
              y[train_idx],
              batch_size=64, epochs=5, verbose=0
          )
          # Tiempo de inferencia (predicción)
          t_infer_start = time.time()
          y_pred = model.predict(
              [X_user[val_idx], X_item[val_idx], X_genre[val_idx],
              X_title[val_idx], X_occ[val_idx], X_age[val_idx]]
          ).flatten()
          t_infer_end = time.time()

          tiempo_pred = (t_infer_end - t_infer_start) / len(val_idx)
          y_pred = model.predict(
              [X_user[val_idx], X_item[val_idx], X_genre[val_idx],
              X_title[val_idx], X_occ[val_idx], X_age[val_idx]]
          ).flatten()

          rmse_folds.append(mean_squared_error(y[val_idx], y_pred, squared=False))
          mae_folds.append(mean_absolute_error(y[val_idx], y_pred))

      resultados_cp.append({
                "activation": act,
                "config": f"C{idx+1}",
                "RMSE": np.mean(rmse_folds),
                "MAE": np.mean(mae_folds),
                "tiempo_pred_s": tiempo_pred,
                **params
            })


  # Mostrar resultados ordenados por RMSE
  df = pd.DataFrame(resultados_cp)
  df_ordenado = df.sort_values(by="RMSE")

  # Mostrar en tabla bonita
  print(tabulate(df_ordenado, headers="keys", tablefmt="fancy_grid", showindex=False))

"""###  Entrenamiento y guardado de los mejores modelos por activación

El siguiente bloque de código entrena y guarda los mejores modelos de red neuronal para un sistema de recomendación basado en múltiples características del usuario y la película. Para ello:

- Se define una configuración óptima para cada función de activación: `relu`, `sigmoid` y `tanh`.
- Para cada activación:
  - Se construye un modelo con la configuración correspondiente (`embedding_dim`, `hidden_units`, `learning_rate` y `dropout`).
  - Se entrena el modelo sobre todos los datos disponibles durante 5 épocas.
  - El modelo entrenado se guarda en un archivo `.h5` cuyo nombre refleja la activación utilizada (por ejemplo: `modelo_mejor_relu.h5`).
- La ruta de cada modelo guardado se almacena en un diccionario llamado `modelos_guardados`.

Esto permite posteriormente cargar y utilizar rápidamente el mejor modelo para cada función de activación sin tener que reentrenarlo.

"""

import os

# Configuraciones óptimas seleccionadas manualmente a partir de la tabla
mejores_configs = {
    "relu":    {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.001,  "dropout": 0.2},
    "sigmoid": {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.001,  "dropout": 0.2},
    "tanh":    {"embedding_dim": 16, "hidden_units": 128, "learning_rate": 0.0005, "dropout": 0.4},
}

modelos_guardados = {}

for act, config in mejores_configs.items():
    print(f" Entrenando mejor modelo para activación: {act}")

    model = build_model(
        n_user_enc, n_item_enc, n_genres, n_titles, n_occs,
        embedding_dim=config["embedding_dim"],
        hidden_units=config["hidden_units"],
        lr=config["learning_rate"],
        act=act,
        dropout_rate=config["dropout"]
    )

    model.fit(
        [X_user, X_item, X_genre, X_title, X_occ, X_age],
        y,
        batch_size=64, epochs=5, verbose=1
    )

    model_path = f"modelo_mejor_{act}.h5"
    model.save(model_path)
    modelos_guardados[act] = model_path

    print(f"✅ Modelo guardado en: {model_path}")

"""###  Recomendación personalizada para un usuario específico

Este bloque de código permite generar recomendaciones personalizadas para un usuario concreto utilizando uno de los modelos previamente entrenados. Las etapas clave son:

1. **Selección de activación**:
   - Se pide al usuario que indique qué función de activación (`relu`, `tanh`, `sigmoid`) se usará.
   - El input se valida hasta que se introduce una opción válida.

2. **Selección de usuario**:
   - Se solicita el ID original del usuario (`userId`) presente en el conjunto de datos.
   - Se valida que el ID exista y sea numérico.

3. **Carga del modelo**:
   - Se carga el modelo correspondiente a la activación seleccionada desde el archivo `.h5`.

4. **Generación de recomendaciones**:
   - Se identifican las películas que el usuario aún no ha visto.
   - Se construyen las entradas necesarias para hacer predicciones (género, título, ocupación, edad, etc., inicializadas como ceros).
   - El modelo predice las puntuaciones para los ítems no vistos.
   - Se muestran las 10 películas mejor puntuadas (no vistas) por su título original.

Este enfoque permite al usuario final recibir recomendaciones específicas basadas en sus características y el historial de visionado.

"""

# Validar activación
activaciones_disponibles = ['relu', 'tanh', 'sigmoid']
while True:
    act = input("¿Qué activación deseas usar? (relu, tanh, sigmoid): ").strip().lower()
    if act in activaciones_disponibles:
        break
    else:
        print(f" Activación inválida. Las opciones válidas son: {activaciones_disponibles}")

# Validar usuario
usuarios_validos = user_enc.classes_
while True:
    try:
        user_id = int(input("Introduce el ID del usuario (userId original): "))
        if user_id in usuarios_validos:
            break
        else:
            print(" Usuario no encontrado en los datos originales.")
    except ValueError:
        print(" Introduce un número entero válido.")

# Codificar usuario y cargar modelo
encoded_user = user_enc.transform([user_id])[0]
model = load_model(f"modelo_mejor_{act}.h5", compile=False)

# Seleccionar ítems no vistos
items_vistos = datos[datos["user"] == encoded_user]["item"].unique()
items_disponibles = np.setdiff1d(np.arange(n_item_enc), items_vistos)

# Preparar inputs
input_user = np.full_like(items_disponibles, fill_value=encoded_user)
input_genre = np.zeros_like(items_disponibles)
input_title = np.zeros_like(items_disponibles)
input_occ = np.zeros_like(items_disponibles)
input_age = np.zeros((len(items_disponibles), 1))

# Predecir y mostrar recomendaciones
predictions = model.predict([
    input_user,
    items_disponibles,
    input_genre,
    input_title,
    input_occ,
    input_age
]).flatten()

top_indices = predictions.argsort()[::-1][:10]
recomendaciones = item_enc.inverse_transform(items_disponibles[top_indices])
titulos_recomendados = Datos[Datos["movieId"].isin(recomendaciones)]["title"].unique()

print(" Títulos recomendados:")
print(titulos_recomendados)